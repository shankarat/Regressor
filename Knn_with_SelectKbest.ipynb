{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# k-Nearest Neighbors\n",
    "\n",
    "K-nearest neighbors (KNN) is a type of supervised learning algorithm which is used for both regression and classification purposes, but mostly it is used for the later. Given a dataset with different classes, KNN tries to predict the correct class of test data by calculating the distance between the test data and all the training points. It then selects the k points which are closest to the test data.  Once the points are selected, the algorithm calculates the probability (in case of classification) of the test point belonging to the classes of the k training points and the class with the highest probability is selected. In the case of a regression problem, the predicted value is the mean of the k selected training points.\n",
    "\n",
    "\n",
    "Let’s understand this with an illustration:\n",
    "\n",
    "\n",
    "1)\tGiven a training dataset as given below. We have a new test data that we need to assign to one of the two classes.\n",
    "\n",
    "<img src=\"1.png\" width=\"\">\n",
    "                                      \n",
    "\n",
    "2)\tNow, the k-NN algorithm calculates the distance between the test data and the given training data.\n",
    "\n",
    "<img src=\"2.png\" width=\"\">\n",
    "\n",
    "                                                        \n",
    "3)\tAfter calculating the distance, it will select the k training points which are nearest to the test data. Let’s assume the value of k is 3 for our example.\n",
    "\n",
    "<img src=\"3.png\" width=\"\">                                            \n",
    "\n",
    "\n",
    "4)\tNow, 3 nearest neighbors are selected, as shown in the figure above. Let’s see in which class our test data will be assigned :\n",
    "\n",
    "Number of Green class values = 2\n",
    "Number of Red class values = 1\n",
    "Probability(Green) = 2/3\n",
    "Probability(Red) = 1/3\n",
    "\n",
    "Since the probability for Green class is higher than Red, the k-NN algorithm will assign the test data to the Green class.\n",
    "\n",
    "Similarly, if this were the case of a regression problem, the predicted value for the test data will simply be the mean of all the 3 nearest values.\n",
    "\n",
    "This is the basic working algorithm for k-NN. Let’s understand how the distance is calculated :\n",
    "\n",
    "### Euclidean Distance: \n",
    "\n",
    "It is the most commonly used method to calculate the distance between two points.\n",
    "The Euclidean distance between two points ‘p(p1,p2)’ and ‘q(q1,q2)’ is calculated  as :\n",
    "\n",
    "<img src=\"4.png\" width=\"\">       image source : Wikipedia\n",
    "\n",
    "<img src=\"5.png\" width=\"\">\n",
    "\n",
    "                                          \n",
    "Similarly,for n-dimensional space, the Euclidean distance is given as :\n",
    "\n",
    "<img src=\"6.png\" width=\"\">\n",
    " \n",
    "\n",
    "\n",
    "### Lazy Learners\n",
    "\n",
    "k-NN algorithms are often termed as Lazy learners. Let’s understand why is that. Most of the algorithms like Bayesian classification, logistic regression, SVM etc., are called Eager learners. These algorithms generalize over the training set before receiving the test data i.e. they create a model based on the training data before receiving the test data and then do the prediction/classification on the test data.\n",
    "But this is not the case with the k-NN algorithm. It doesn’t create a generalized model for the training set but waits for the test data. Once test data is provided then only it starts generalizing the training data to classify the test data.  So, a lazy learner just stores the training data and waits for the test set. Such algorithms work less while training and more while classifying a given test dataset.\n",
    "\n",
    "\n",
    "\n",
    "### Pros and Cons of k-NN Algorithm\n",
    "\n",
    "Pros:\n",
    "*\tIt can be used for both regression and classification problems.\n",
    "*\tIt is very simple and easy to implement.\n",
    "*\tMathematics behind the algorithm is easy to understand.\n",
    "*\tThere is no need to create model or do hyperparameter tuning.\n",
    "*   KNN doesn't make any assumption for the distribution of the given data.\n",
    "*   There is not much time cost in training phase.\n",
    "\n",
    "Cons:\n",
    "*\tFinding the optimum value of ‘k’\n",
    "*\tIt takes a lot of time to compute the distance between each test sample and all training samples.\n",
    "*\tSince the model is not saved beforehand in this algorithm (lazy learner), so every time one predicts a test value, it follows the same steps again and again. \n",
    "*\tSince, we need to store the whole training set for every test set, it requires a lot of space.\n",
    "*\tIt is not suitable for high dimensional data.\n",
    "*   Expensive in testing phase\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Different ways to perform k-NN\n",
    "\n",
    "Above we studied the way k-NN classifies the data by calculating the distance of test data from each of the observations and selecting ‘k’ values. This approach is also known as “Brute Force k-NN”.  This is computionally very expensive. So, there are other ways as well to perfrom k-NN which are comparatively less expensive than Brute force approach. The idea behind using other algorithms for k-NN classifier is to reduce the time during test period by preprocessing the training data in such a way that the test data can be easily classified in the appropriate clusters.\n",
    "\n",
    "\n",
    "Couple more algorithms are k-Dimensional Tree (kd tree) and Ball tree\n",
    " \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Python  Implementation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<img src=\"image.jpg\" width=\"\">"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Import necessory libraries\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import  confusion_matrix,classification_report\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn.preprocessing import StandardScaler\n",
    "import seaborn as sns\n",
    "import matplotlib.pyplot as plt\n",
    "#import plotly \n",
    "\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>diagnosis</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "      <th>Unnamed: 32</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>842302</th>\n",
       "      <td>M</td>\n",
       "      <td>17.99</td>\n",
       "      <td>10.38</td>\n",
       "      <td>122.80</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.11840</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.3001</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>0.2419</td>\n",
       "      <td>...</td>\n",
       "      <td>17.33</td>\n",
       "      <td>184.60</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>0.1622</td>\n",
       "      <td>0.6656</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.2654</td>\n",
       "      <td>0.4601</td>\n",
       "      <td>0.11890</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>842517</th>\n",
       "      <td>M</td>\n",
       "      <td>20.57</td>\n",
       "      <td>17.77</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08474</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.0869</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>0.1812</td>\n",
       "      <td>...</td>\n",
       "      <td>23.41</td>\n",
       "      <td>158.80</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>0.1238</td>\n",
       "      <td>0.1866</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.1860</td>\n",
       "      <td>0.2750</td>\n",
       "      <td>0.08902</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84300903</th>\n",
       "      <td>M</td>\n",
       "      <td>19.69</td>\n",
       "      <td>21.25</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.10960</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.1974</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>0.2069</td>\n",
       "      <td>...</td>\n",
       "      <td>25.53</td>\n",
       "      <td>152.50</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>0.1444</td>\n",
       "      <td>0.4245</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.2430</td>\n",
       "      <td>0.3613</td>\n",
       "      <td>0.08758</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84348301</th>\n",
       "      <td>M</td>\n",
       "      <td>11.42</td>\n",
       "      <td>20.38</td>\n",
       "      <td>77.58</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.14250</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.2414</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>0.2597</td>\n",
       "      <td>...</td>\n",
       "      <td>26.50</td>\n",
       "      <td>98.87</td>\n",
       "      <td>567.7</td>\n",
       "      <td>0.2098</td>\n",
       "      <td>0.8663</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.2575</td>\n",
       "      <td>0.6638</td>\n",
       "      <td>0.17300</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84358402</th>\n",
       "      <td>M</td>\n",
       "      <td>20.29</td>\n",
       "      <td>14.34</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.10030</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.1980</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>0.1809</td>\n",
       "      <td>...</td>\n",
       "      <td>16.67</td>\n",
       "      <td>152.20</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>0.1374</td>\n",
       "      <td>0.2050</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.1625</td>\n",
       "      <td>0.2364</td>\n",
       "      <td>0.07678</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "         diagnosis  radius_mean  texture_mean  perimeter_mean  area_mean  \\\n",
       "id                                                                         \n",
       "842302           M        17.99         10.38          122.80     1001.0   \n",
       "842517           M        20.57         17.77          132.90     1326.0   \n",
       "84300903         M        19.69         21.25          130.00     1203.0   \n",
       "84348301         M        11.42         20.38           77.58      386.1   \n",
       "84358402         M        20.29         14.34          135.10     1297.0   \n",
       "\n",
       "          smoothness_mean  compactness_mean  concavity_mean  \\\n",
       "id                                                            \n",
       "842302            0.11840           0.27760          0.3001   \n",
       "842517            0.08474           0.07864          0.0869   \n",
       "84300903          0.10960           0.15990          0.1974   \n",
       "84348301          0.14250           0.28390          0.2414   \n",
       "84358402          0.10030           0.13280          0.1980   \n",
       "\n",
       "          concave points_mean  symmetry_mean  ...  texture_worst  \\\n",
       "id                                            ...                  \n",
       "842302                0.14710         0.2419  ...          17.33   \n",
       "842517                0.07017         0.1812  ...          23.41   \n",
       "84300903              0.12790         0.2069  ...          25.53   \n",
       "84348301              0.10520         0.2597  ...          26.50   \n",
       "84358402              0.10430         0.1809  ...          16.67   \n",
       "\n",
       "          perimeter_worst  area_worst  smoothness_worst  compactness_worst  \\\n",
       "id                                                                           \n",
       "842302             184.60      2019.0            0.1622             0.6656   \n",
       "842517             158.80      1956.0            0.1238             0.1866   \n",
       "84300903           152.50      1709.0            0.1444             0.4245   \n",
       "84348301            98.87       567.7            0.2098             0.8663   \n",
       "84358402           152.20      1575.0            0.1374             0.2050   \n",
       "\n",
       "          concavity_worst  concave points_worst  symmetry_worst  \\\n",
       "id                                                                \n",
       "842302             0.7119                0.2654          0.4601   \n",
       "842517             0.2416                0.1860          0.2750   \n",
       "84300903           0.4504                0.2430          0.3613   \n",
       "84348301           0.6869                0.2575          0.6638   \n",
       "84358402           0.4000                0.1625          0.2364   \n",
       "\n",
       "          fractal_dimension_worst  Unnamed: 32  \n",
       "id                                              \n",
       "842302                    0.11890          NaN  \n",
       "842517                    0.08902          NaN  \n",
       "84300903                  0.08758          NaN  \n",
       "84348301                  0.17300          NaN  \n",
       "84358402                  0.07678          NaN  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Get the CSV data here and print head\n",
    "df = pd.read_csv('breast cancer.csv',index_col=0)\n",
    "df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Shape        ------> (569, 32)\n",
      "Each Column and data type and its count \n",
      "\n",
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Int64Index: 569 entries, 842302 to 92751\n",
      "Data columns (total 32 columns):\n",
      " #   Column                   Non-Null Count  Dtype  \n",
      "---  ------                   --------------  -----  \n",
      " 0   diagnosis                569 non-null    object \n",
      " 1   radius_mean              569 non-null    float64\n",
      " 2   texture_mean             569 non-null    float64\n",
      " 3   perimeter_mean           569 non-null    float64\n",
      " 4   area_mean                569 non-null    float64\n",
      " 5   smoothness_mean          569 non-null    float64\n",
      " 6   compactness_mean         569 non-null    float64\n",
      " 7   concavity_mean           569 non-null    float64\n",
      " 8   concave points_mean      569 non-null    float64\n",
      " 9   symmetry_mean            569 non-null    float64\n",
      " 10  fractal_dimension_mean   569 non-null    float64\n",
      " 11  radius_se                569 non-null    float64\n",
      " 12  texture_se               569 non-null    float64\n",
      " 13  perimeter_se             569 non-null    float64\n",
      " 14  area_se                  569 non-null    float64\n",
      " 15  smoothness_se            569 non-null    float64\n",
      " 16  compactness_se           569 non-null    float64\n",
      " 17  concavity_se             569 non-null    float64\n",
      " 18  concave points_se        569 non-null    float64\n",
      " 19  symmetry_se              569 non-null    float64\n",
      " 20  fractal_dimension_se     569 non-null    float64\n",
      " 21  radius_worst             569 non-null    float64\n",
      " 22  texture_worst            569 non-null    float64\n",
      " 23  perimeter_worst          569 non-null    float64\n",
      " 24  area_worst               569 non-null    float64\n",
      " 25  smoothness_worst         569 non-null    float64\n",
      " 26  compactness_worst        569 non-null    float64\n",
      " 27  concavity_worst          569 non-null    float64\n",
      " 28  concave points_worst     569 non-null    float64\n",
      " 29  symmetry_worst           569 non-null    float64\n",
      " 30  fractal_dimension_worst  569 non-null    float64\n",
      " 31  Unnamed: 32              0 non-null      float64\n",
      "dtypes: float64(31), object(1)\n",
      "memory usage: 146.7+ KB\n",
      "None\n"
     ]
    }
   ],
   "source": [
    "#print summary\n",
    "print ('Shape        ------>', df.shape)\n",
    "print ('Each Column and data type and its count','\\n')\n",
    "print ( df.info())"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#DROP ALERT 1 : Unnamed:32 column has all nulls. Safe to remove the column.\n",
    "df = df.drop(['Unnamed: 32'],axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>texture_mean</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>smoothness_mean</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>symmetry_mean</th>\n",
       "      <th>fractal_dimension_mean</th>\n",
       "      <th>...</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>symmetry_worst</th>\n",
       "      <th>fractal_dimension_worst</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>...</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "      <td>569.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>14.127292</td>\n",
       "      <td>19.289649</td>\n",
       "      <td>91.969033</td>\n",
       "      <td>654.889104</td>\n",
       "      <td>0.096360</td>\n",
       "      <td>0.104341</td>\n",
       "      <td>0.088799</td>\n",
       "      <td>0.048919</td>\n",
       "      <td>0.181162</td>\n",
       "      <td>0.062798</td>\n",
       "      <td>...</td>\n",
       "      <td>16.269190</td>\n",
       "      <td>25.677223</td>\n",
       "      <td>107.261213</td>\n",
       "      <td>880.583128</td>\n",
       "      <td>0.132369</td>\n",
       "      <td>0.254265</td>\n",
       "      <td>0.272188</td>\n",
       "      <td>0.114606</td>\n",
       "      <td>0.290076</td>\n",
       "      <td>0.083946</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>3.524049</td>\n",
       "      <td>4.301036</td>\n",
       "      <td>24.298981</td>\n",
       "      <td>351.914129</td>\n",
       "      <td>0.014064</td>\n",
       "      <td>0.052813</td>\n",
       "      <td>0.079720</td>\n",
       "      <td>0.038803</td>\n",
       "      <td>0.027414</td>\n",
       "      <td>0.007060</td>\n",
       "      <td>...</td>\n",
       "      <td>4.833242</td>\n",
       "      <td>6.146258</td>\n",
       "      <td>33.602542</td>\n",
       "      <td>569.356993</td>\n",
       "      <td>0.022832</td>\n",
       "      <td>0.157336</td>\n",
       "      <td>0.208624</td>\n",
       "      <td>0.065732</td>\n",
       "      <td>0.061867</td>\n",
       "      <td>0.018061</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>6.981000</td>\n",
       "      <td>9.710000</td>\n",
       "      <td>43.790000</td>\n",
       "      <td>143.500000</td>\n",
       "      <td>0.052630</td>\n",
       "      <td>0.019380</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.106000</td>\n",
       "      <td>0.049960</td>\n",
       "      <td>...</td>\n",
       "      <td>7.930000</td>\n",
       "      <td>12.020000</td>\n",
       "      <td>50.410000</td>\n",
       "      <td>185.200000</td>\n",
       "      <td>0.071170</td>\n",
       "      <td>0.027290</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.000000</td>\n",
       "      <td>0.156500</td>\n",
       "      <td>0.055040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>11.700000</td>\n",
       "      <td>16.170000</td>\n",
       "      <td>75.170000</td>\n",
       "      <td>420.300000</td>\n",
       "      <td>0.086370</td>\n",
       "      <td>0.064920</td>\n",
       "      <td>0.029560</td>\n",
       "      <td>0.020310</td>\n",
       "      <td>0.161900</td>\n",
       "      <td>0.057700</td>\n",
       "      <td>...</td>\n",
       "      <td>13.010000</td>\n",
       "      <td>21.080000</td>\n",
       "      <td>84.110000</td>\n",
       "      <td>515.300000</td>\n",
       "      <td>0.116600</td>\n",
       "      <td>0.147200</td>\n",
       "      <td>0.114500</td>\n",
       "      <td>0.064930</td>\n",
       "      <td>0.250400</td>\n",
       "      <td>0.071460</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>13.370000</td>\n",
       "      <td>18.840000</td>\n",
       "      <td>86.240000</td>\n",
       "      <td>551.100000</td>\n",
       "      <td>0.095870</td>\n",
       "      <td>0.092630</td>\n",
       "      <td>0.061540</td>\n",
       "      <td>0.033500</td>\n",
       "      <td>0.179200</td>\n",
       "      <td>0.061540</td>\n",
       "      <td>...</td>\n",
       "      <td>14.970000</td>\n",
       "      <td>25.410000</td>\n",
       "      <td>97.660000</td>\n",
       "      <td>686.500000</td>\n",
       "      <td>0.131300</td>\n",
       "      <td>0.211900</td>\n",
       "      <td>0.226700</td>\n",
       "      <td>0.099930</td>\n",
       "      <td>0.282200</td>\n",
       "      <td>0.080040</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>15.780000</td>\n",
       "      <td>21.800000</td>\n",
       "      <td>104.100000</td>\n",
       "      <td>782.700000</td>\n",
       "      <td>0.105300</td>\n",
       "      <td>0.130400</td>\n",
       "      <td>0.130700</td>\n",
       "      <td>0.074000</td>\n",
       "      <td>0.195700</td>\n",
       "      <td>0.066120</td>\n",
       "      <td>...</td>\n",
       "      <td>18.790000</td>\n",
       "      <td>29.720000</td>\n",
       "      <td>125.400000</td>\n",
       "      <td>1084.000000</td>\n",
       "      <td>0.146000</td>\n",
       "      <td>0.339100</td>\n",
       "      <td>0.382900</td>\n",
       "      <td>0.161400</td>\n",
       "      <td>0.317900</td>\n",
       "      <td>0.092080</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>28.110000</td>\n",
       "      <td>39.280000</td>\n",
       "      <td>188.500000</td>\n",
       "      <td>2501.000000</td>\n",
       "      <td>0.163400</td>\n",
       "      <td>0.345400</td>\n",
       "      <td>0.426800</td>\n",
       "      <td>0.201200</td>\n",
       "      <td>0.304000</td>\n",
       "      <td>0.097440</td>\n",
       "      <td>...</td>\n",
       "      <td>36.040000</td>\n",
       "      <td>49.540000</td>\n",
       "      <td>251.200000</td>\n",
       "      <td>4254.000000</td>\n",
       "      <td>0.222600</td>\n",
       "      <td>1.058000</td>\n",
       "      <td>1.252000</td>\n",
       "      <td>0.291000</td>\n",
       "      <td>0.663800</td>\n",
       "      <td>0.207500</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>8 rows × 30 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       radius_mean  texture_mean  perimeter_mean    area_mean  \\\n",
       "count   569.000000    569.000000      569.000000   569.000000   \n",
       "mean     14.127292     19.289649       91.969033   654.889104   \n",
       "std       3.524049      4.301036       24.298981   351.914129   \n",
       "min       6.981000      9.710000       43.790000   143.500000   \n",
       "25%      11.700000     16.170000       75.170000   420.300000   \n",
       "50%      13.370000     18.840000       86.240000   551.100000   \n",
       "75%      15.780000     21.800000      104.100000   782.700000   \n",
       "max      28.110000     39.280000      188.500000  2501.000000   \n",
       "\n",
       "       smoothness_mean  compactness_mean  concavity_mean  concave points_mean  \\\n",
       "count       569.000000        569.000000      569.000000           569.000000   \n",
       "mean          0.096360          0.104341        0.088799             0.048919   \n",
       "std           0.014064          0.052813        0.079720             0.038803   \n",
       "min           0.052630          0.019380        0.000000             0.000000   \n",
       "25%           0.086370          0.064920        0.029560             0.020310   \n",
       "50%           0.095870          0.092630        0.061540             0.033500   \n",
       "75%           0.105300          0.130400        0.130700             0.074000   \n",
       "max           0.163400          0.345400        0.426800             0.201200   \n",
       "\n",
       "       symmetry_mean  fractal_dimension_mean  ...  radius_worst  \\\n",
       "count     569.000000              569.000000  ...    569.000000   \n",
       "mean        0.181162                0.062798  ...     16.269190   \n",
       "std         0.027414                0.007060  ...      4.833242   \n",
       "min         0.106000                0.049960  ...      7.930000   \n",
       "25%         0.161900                0.057700  ...     13.010000   \n",
       "50%         0.179200                0.061540  ...     14.970000   \n",
       "75%         0.195700                0.066120  ...     18.790000   \n",
       "max         0.304000                0.097440  ...     36.040000   \n",
       "\n",
       "       texture_worst  perimeter_worst   area_worst  smoothness_worst  \\\n",
       "count     569.000000       569.000000   569.000000        569.000000   \n",
       "mean       25.677223       107.261213   880.583128          0.132369   \n",
       "std         6.146258        33.602542   569.356993          0.022832   \n",
       "min        12.020000        50.410000   185.200000          0.071170   \n",
       "25%        21.080000        84.110000   515.300000          0.116600   \n",
       "50%        25.410000        97.660000   686.500000          0.131300   \n",
       "75%        29.720000       125.400000  1084.000000          0.146000   \n",
       "max        49.540000       251.200000  4254.000000          0.222600   \n",
       "\n",
       "       compactness_worst  concavity_worst  concave points_worst  \\\n",
       "count         569.000000       569.000000            569.000000   \n",
       "mean            0.254265         0.272188              0.114606   \n",
       "std             0.157336         0.208624              0.065732   \n",
       "min             0.027290         0.000000              0.000000   \n",
       "25%             0.147200         0.114500              0.064930   \n",
       "50%             0.211900         0.226700              0.099930   \n",
       "75%             0.339100         0.382900              0.161400   \n",
       "max             1.058000         1.252000              0.291000   \n",
       "\n",
       "       symmetry_worst  fractal_dimension_worst  \n",
       "count      569.000000               569.000000  \n",
       "mean         0.290076                 0.083946  \n",
       "std          0.061867                 0.018061  \n",
       "min          0.156500                 0.055040  \n",
       "25%          0.250400                 0.071460  \n",
       "50%          0.282200                 0.080040  \n",
       "75%          0.317900                 0.092080  \n",
       "max          0.663800                 0.207500  \n",
       "\n",
       "[8 rows x 30 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Dataframe statistics\n",
    "df.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "diagnosis                  0\n",
       "radius_mean                0\n",
       "texture_mean               0\n",
       "perimeter_mean             0\n",
       "area_mean                  0\n",
       "smoothness_mean            0\n",
       "compactness_mean           0\n",
       "concavity_mean             0\n",
       "concave points_mean        0\n",
       "symmetry_mean              0\n",
       "fractal_dimension_mean     0\n",
       "radius_se                  0\n",
       "texture_se                 0\n",
       "perimeter_se               0\n",
       "area_se                    0\n",
       "smoothness_se              0\n",
       "compactness_se             0\n",
       "concavity_se               0\n",
       "concave points_se          0\n",
       "symmetry_se                0\n",
       "fractal_dimension_se       0\n",
       "radius_worst               0\n",
       "texture_worst              0\n",
       "perimeter_worst            0\n",
       "area_worst                 0\n",
       "smoothness_worst           0\n",
       "compactness_worst          0\n",
       "concavity_worst            0\n",
       "concave points_worst       0\n",
       "symmetry_worst             0\n",
       "fractal_dimension_worst    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Seems no other cols have nulls. It's safe to proceed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "B    357\n",
       "M    212\n",
       "Name: diagnosis, dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.diagnosis.value_counts()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Belign Tumor (B)    => 357\n",
      "Malignant Tumor (M) => 212\n"
     ]
    }
   ],
   "source": [
    "\n",
    "print ('Belign Tumor (B)    =>',df.diagnosis.value_counts()[0])\n",
    "print ('Malignant Tumor (M) =>',df.diagnosis.value_counts()[1])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Dataset is not imbalanced, we are good to proceed."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "iVBORw0KGgoAAAANSUhEUgAAAYUAAAEGCAYAAACKB4k+AAAABHNCSVQICAgIfAhkiAAAAAlwSFlzAAALEgAACxIB0t1+/AAAADh0RVh0U29mdHdhcmUAbWF0cGxvdGxpYiB2ZXJzaW9uMy4yLjEsIGh0dHA6Ly9tYXRwbG90bGliLm9yZy+j8jraAAASYklEQVR4nO3df4xdZ33n8fcnTppES1oSeZI1tqld5C7rpI2zTL1sUVsKbZOl23WCADlSWXc3kvkjSFC1KyVdLaRU1tI2FFW0QXJKwCBKam1g46KUbbCgLIKNmUQmsR0sLJImxm48/ExCW69svvvHPX5yGY/tsZMzdzL3/ZKu7jnPeZ5zvhM585nnnHPPTVUhSRLAeaMuQJK0cBgKkqTGUJAkNYaCJKkxFCRJzfmjLuD5WLp0aa1atWrUZUjSi8qDDz74raqamG3bizoUVq1axdTU1KjLkKQXlSR/f6ptnj6SJDWGgiSpMRQkSY2hIElqDAVJUmMoSJIaQ0GS1BgKkqTGUJAkNS/qTzRLi9kT7/mZUZegBejl73qk1/33NlNIclGSXUm+mmRvkt/v2m9L8s0ku7vXG4bG3JrkQJL9Sa7tqzZJ0uz6nCkcBV5XVc8muQD4YpK/6ba9v6puH+6cZC2wEbgSeBnw2SQ/XVXHe6xRkjSkt5lCDTzbrV7QvU73hdAbgLur6mhVPQYcANb3VZ8k6WS9XmhOsiTJbuAIcH9VPdBtenuSh5PcleTSrm058OTQ8INd28x9bk4ylWRqenq6z/Ilaez0GgpVdbyq1gErgPVJrgI+CLwCWAccBt7Xdc9su5hln1urarKqJicmZn0cuCTpHM3LLalV9T3g88B1VfVUFxY/BO7kuVNEB4GVQ8NWAIfmoz5J0kCfdx9NJHlpt3wx8CvA15IsG+p2A7CnW94BbExyYZLVwBpgV1/1SZJO1ufdR8uAbUmWMAif7VX16SQfS7KOwamhx4G3AVTV3iTbgX3AMeBm7zySpPnVWyhU1cPANbO0v/U0Y7YAW/qqSZJ0ej7mQpLUGAqSpMZQkCQ1hoIkqTEUJEmNoSBJagwFSVJjKEiSGkNBktQYCpKkxlCQJDWGgiSpMRQkSY2hIElqDAVJUmMoSJIaQ0GS1BgKkqTGUJAkNYaCJKnpLRSSXJRkV5KvJtmb5Pe79suS3J/k6937pUNjbk1yIMn+JNf2VZskaXZ9zhSOAq+rqquBdcB1SV4N3ALsrKo1wM5unSRrgY3AlcB1wB1JlvRYnyRpht5CoQae7VYv6F4FbAC2de3bgOu75Q3A3VV1tKoeAw4A6/uqT5J0sl6vKSRZkmQ3cAS4v6oeAK6oqsMA3fvlXfflwJNDww92bTP3uTnJVJKp6enpPsuXpLHTayhU1fGqWgesANYnueo03TPbLmbZ59aqmqyqyYmJiReqVEkS83T3UVV9D/g8g2sFTyVZBtC9H+m6HQRWDg1bARyaj/okSQN93n00keSl3fLFwK8AXwN2AJu6bpuAe7vlHcDGJBcmWQ2sAXb1VZ8k6WTn97jvZcC27g6i84DtVfXpJF8Gtie5CXgCeDNAVe1Nsh3YBxwDbq6q4z3WJ0maobdQqKqHgWtmaf828PpTjNkCbOmrJknS6fmJZklSYyhIkhpDQZLUGAqSpMZQkCQ1hoIkqTEUJEmNoSBJagwFSVJjKEiSGkNBktQYCpKkxlCQJDWGgiSpMRQkSY2hIElqDAVJUmMoSJIaQ0GS1BgKkqSmt1BIsjLJ55I8mmRvknd07bcl+WaS3d3rDUNjbk1yIMn+JNf2VZskaXbn97jvY8DvVNVDSS4BHkxyf7ft/VV1+3DnJGuBjcCVwMuAzyb56ao63mONkqQhvc0UqupwVT3ULT8DPAosP82QDcDdVXW0qh4DDgDr+6pPknSyebmmkGQVcA3wQNf09iQPJ7kryaVd23LgyaFhB5klRJJsTjKVZGp6errHqiVp/PQeCkleAtwDvLOqngY+CLwCWAccBt53oussw+ukhqqtVTVZVZMTExM9VS1J46nXUEhyAYNA+HhVfRKgqp6qquNV9UPgTp47RXQQWDk0fAVwqM/6JEk/qs+7jwJ8CHi0qv5kqH3ZULcbgD3d8g5gY5ILk6wG1gC7+qpPknSyPu8+eg3wVuCRJLu7tt8DbkyyjsGpoceBtwFU1d4k24F9DO5cutk7jyRpfvUWClX1RWa/TnDfacZsAbb0VZMk6fT8RLMkqTEUJEmNoSBJagwFSVJjKEiSGkNBktQYCpKkxlCQJDWGgiSpMRQkSY2hIElqDAVJUmMoSJIaQ0GS1BgKkqTGUJAkNX1+89qLwqv+60dHXYIWoAf/+D+NugRpJJwpSJIaQ0GS1MwpFJLsnEubJOnF7bShkOSiJJcBS5NcmuSy7rUKeNkZxq5M8rkkjybZm+QdXftlSe5P8vXu/dKhMbcmOZBkf5Jrn/+PJ0k6G2eaKbwNeBB4Zfd+4nUv8OdnGHsM+J2q+tfAq4Gbk6wFbgF2VtUaYGe3TrdtI3AlcB1wR5Il5/JDSZLOzWlDoar+tKpWA79bVT9VVau719VV9WdnGHu4qh7qlp8BHgWWAxuAbV23bcD13fIG4O6qOlpVjwEHgPXn/JNJks7anG5JraoPJPl5YNXwmKqa0/2c3emma4AHgCuq6nA3/nCSy7tuy4H/OzTsYNc2c1+bgc0AL3/5y+dyeEnSHM0pFJJ8DHgFsBs43jUXcMZQSPIS4B7gnVX1dJJTdp2lrU5qqNoKbAWYnJw8absk6dzN9cNrk8DaqjqrX8JJLmAQCB+vqk92zU8lWdbNEpYBR7r2g8DKoeErgENnczxJ0vMz188p7AH+5dnsOIMpwYeAR6vqT4Y27QA2dcubGFy0PtG+McmFSVYDa4BdZ3NMSdLzM9eZwlJgX5JdwNETjVX1H08z5jXAW4FHkuzu2n4PeC+wPclNwBPAm7t97U2yHdjH4M6lm6vq+Mm7lST1Za6hcNvZ7riqvsjs1wkAXn+KMVuALWd7LEnSC2Oudx/9Xd+FSJJGb653Hz3Dc3cC/RhwAfCDqvrxvgqTJM2/uc4ULhleT3I9frBMkhadc3pKalX9L+B1L3AtkqQRm+vpozcOrZ7H4HMLfnBMkhaZud599BtDy8eAxxk8q0iStIjM9ZrCf+67EEnS6M31S3ZWJPlUkiNJnkpyT5IVfRcnSZpfc73Q/GEGj6F4GYMnl/511yZJWkTmGgoTVfXhqjrWvT4CTPRYlyRpBOYaCt9K8ptJlnSv3wS+3WdhkqT5N9dQ+C/AW4B/AA4DbwK8+CxJi8xcb0n9A2BTVX0XIMllwO0MwkKStEjMdabwsycCAaCqvsPg6zUlSYvIXEPhvCSXnljpZgpznWVIkl4k5vqL/X3Al5L8TwaPt3gLfu+BJC06c/1E80eTTDF4CF6AN1bVvl4rkyTNuzmfAupCwCCQpEXsnB6dLUlanAwFSVLTWygkuat7gN6eobbbknwzye7u9YahbbcmOZBkf5Jr+6pLknRqfc4UPgJcN0v7+6tqXfe6DyDJWmAjcGU35o4kS3qsTZI0i95Coaq+AHxnjt03AHdX1dGqegw4gN8BLUnzbhTXFN6e5OHu9NKJD8QtB54c6nOwaztJks1JppJMTU9P912rJI2V+Q6FDwKvANYxeLDe+7r2zNJ31u+ArqqtVTVZVZMTEz69W5JeSPMaClX1VFUdr6ofAnfy3Cmig8DKoa4rgEPzWZskaZ5DIcmyodUbgBN3Ju0ANia5MMlqYA2waz5rkyT1+FC7JJ8AXgssTXIQeDfw2iTrGJwaehx4G0BV7U2yncEnpo8BN1fV8b5qkyTNrrdQqKobZ2n+0Gn6b8GH7EnSSPmJZklSYyhIkhpDQZLUGAqSpMZQkCQ1hoIkqTEUJEmNoSBJagwFSVJjKEiSGkNBktQYCpKkxlCQJDWGgiSpMRQkSY2hIElqDAVJUmMoSJIaQ0GS1BgKkqSmt1BIcleSI0n2DLVdluT+JF/v3i8d2nZrkgNJ9ie5tq+6JEmn1udM4SPAdTPabgF2VtUaYGe3TpK1wEbgym7MHUmW9FibJGkWvYVCVX0B+M6M5g3Atm55G3D9UPvdVXW0qh4DDgDr+6pNkjS7+b6mcEVVHQbo3i/v2pcDTw71O9i1nSTJ5iRTSaamp6d7LVaSxs1CudCcWdpqto5VtbWqJqtqcmJioueyJGm8zHcoPJVkGUD3fqRrPwisHOq3Ajg0z7VJ0tib71DYAWzqljcB9w61b0xyYZLVwBpg1zzXJklj7/y+dpzkE8BrgaVJDgLvBt4LbE9yE/AE8GaAqtqbZDuwDzgG3FxVx/uqTZI0u95CoapuPMWm15+i/xZgS1/1SJLObKFcaJYkLQCGgiSpMRQkSY2hIElqDAVJUmMoSJIaQ0GS1BgKkqTGUJAkNYaCJKkxFCRJjaEgSWoMBUlSYyhIkhpDQZLUGAqSpMZQkCQ1hoIkqTEUJEmNoSBJas4fxUGTPA48AxwHjlXVZJLLgL8CVgGPA2+pqu+Ooj5JGlejnCn8clWtq6rJbv0WYGdVrQF2duuSpHm0kE4fbQC2dcvbgOtHWIskjaVRhUIBf5vkwSSbu7YrquowQPd++WwDk2xOMpVkanp6ep7KlaTxMJJrCsBrqupQksuB+5N8ba4Dq2orsBVgcnKy+ipQksbRSGYKVXWoez8CfApYDzyVZBlA935kFLVJ0jib91BI8i+SXHJiGfg1YA+wA9jUddsE3DvftUnSuBvF6aMrgE8lOXH8v6yqzyT5CrA9yU3AE8CbR1CbJI21eQ+FqvoGcPUs7d8GXj/f9UiSnrOQbkmVJI2YoSBJagwFSVJjKEiSGkNBktQYCpKkxlCQJDWGgiSpMRQkSY2hIElqDAVJUmMoSJIaQ0GS1BgKkqTGUJAkNYaCJKkxFCRJjaEgSWoMBUlSYyhIkpoFFwpJrkuyP8mBJLeMuh5JGicLKhSSLAH+HPj3wFrgxiRrR1uVJI2PBRUKwHrgQFV9o6r+H3A3sGHENUnS2Dh/1AXMsBx4cmj9IPBvhzsk2Qxs7lafTbJ/nmobB0uBb426iIUgt28adQn6Uf7bPOHdeSH28pOn2rDQQmG2n7Z+ZKVqK7B1fsoZL0mmqmpy1HVIM/lvc/4stNNHB4GVQ+srgEMjqkWSxs5CC4WvAGuSrE7yY8BGYMeIa5KksbGgTh9V1bEkbwf+N7AEuKuq9o64rHHiaTktVP7bnCepqjP3kiSNhYV2+kiSNEKGgiSpMRTGXJJK8rGh9fOTTCf59CjrkgCSHE+yO8lXkzyU5OdHXdNit6AuNGskfgBcleTiqvon4FeBb464JumEf6qqdQBJrgX+B/BLoy1pcXOmIIC/AX69W74R+MQIa5FO5ceB7466iMXOUBAMnjG1MclFwM8CD4y4HumEi7vTR18D/gL4g1EXtNh5+khU1cNJVjGYJdw32mqkHzF8+ujfAR9NclV5L31vnCnohB3A7XjqSAtUVX2ZwYPxJkZdy2LmTEEn3AV8v6oeSfLaURcjzZTklQyedPDtUdeymBkKAqCqDgJ/Ouo6pBkuTrK7Ww6wqaqOj7Kgxc7HXEiSGq8pSJIaQ0GS1BgKkqTGUJAkNYaCJKnxllSpk+Q24FkGz9j5QlV9doS1vGfUNWg8GQrSDFX1LmvQuPL0kcZakv+WZH+SzwL/qmv7SJI3dcvvSvKVJHuSbE2Srv3nkjyc5MtJ/jjJnq79t5J8Mslnknw9yR8NHevGJI90+/rDrm1Jd7w93bbfnqWG9ybZ1x3v9nn9D6Sx40xBYyvJq4CNwDUM/l94CHhwRrc/q6r3dP0/BvwH4K+BDwObq+pLSd47Y8y6bp9Hgf1JPgAcB/4QeBWDxz//bZLrgSeB5VV1VXeMl86o8TLgBuCVVVUzt0svNGcKGme/AHyqqv6xqp5m8FDAmX45yQNJHgFeB1zZ/WK+pKq+1PX5yxljdlbV96vqn4F9wE8CPwd8vqqmq+oY8HHgF4FvAD+V5ANJrgOenrGvp4F/Bv4iyRuBf3zeP7V0GoaCxt0pn/PSfb/EHcCbqupngDuBixg8g+d0jg4tH2cwC5l1TFV9F7ga+DxwM4PvDBjefgxYD9wDXA985gzHlp4XQ0Hj7AvADUkuTnIJ8Bsztl/UvX8ryUuAN0H7Rf5Mkld32zfO4VgPAL+UZGmSJQy+u+LvkiwFzquqe4D/Dvyb4UHdcX+iqu4D3sng1JTUG68paGxV1UNJ/grYDfw98H9mbP9ekjuBR4DHga8Mbb4JuDPJDxj8lf/9MxzrcJJbgc8xmDXcV1X3Jrka+HCSE3+g3Tpj6CXAvd2sJcBvn/UPKp0Fn5IqnYMkL6mqZ7vlW4BlVfWOEZclPW/OFKRz8+vdX/7nM5hl/NZoy5FeGM4UJEmNF5olSY2hIElqDAVJUmMoSJIaQ0GS1Px/nU/oS5OQ23IAAAAASUVORK5CYII=\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "#Plot each class freequency\n",
    "sns.countplot(x='diagnosis',data=df)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(569, 31)"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.shape"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Using SelectKBest feature Selection method\n",
    "\n",
    "####  SelectKBest use f_classif function to find best features, where f_classif uses ANOVA test."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.feature_selection import SelectKBest, f_classif"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "#Replace Label column (diagnosis) into binary codes\n",
    "df['diagnosis'] = df['diagnosis'].replace({'M':1,'B':0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "X = df.drop('diagnosis', axis=1)\n",
    "y = df.diagnosis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "            Feature_Name       Score\n",
      "27  concave points_worst  964.385393\n",
      "22       perimeter_worst  897.944219\n",
      "7    concave points_mean  861.676020\n",
      "20          radius_worst  860.781707\n",
      "2         perimeter_mean  697.235272\n",
      "23            area_worst  661.600206\n",
      "0            radius_mean  646.981021\n",
      "3              area_mean  573.060747\n",
      "6         concavity_mean  533.793126\n",
      "26       concavity_worst  436.691939\n",
      "5       compactness_mean  313.233079\n",
      "25     compactness_worst  304.341063\n",
      "10             radius_se  268.840327\n",
      "12          perimeter_se  253.897392\n",
      "13               area_se  243.651586\n",
      "21         texture_worst  149.596905\n",
      "24      smoothness_worst  122.472880\n"
     ]
    }
   ],
   "source": [
    "best_features = SelectKBest(score_func=f_classif, k=17)\n",
    "fit = best_features.fit(X,y)\n",
    "df_scores = pd.DataFrame(fit.scores_)\n",
    "df_columns = pd.DataFrame(X.columns)\n",
    "\n",
    "# concatenate dataframes\n",
    "feature_scores = pd.concat([df_columns, df_scores],axis=1)\n",
    "feature_scores.columns = ['Feature_Name','Score']  # name output columns\n",
    "\n",
    "print(feature_scores.nlargest(17,'Score'))  # print 17 best features\n",
    "\n",
    "\n",
    "# Export selected features to .csv for later use\n",
    "#df_backup = feature_scores.nlargest(17,'Score')\n",
    "#df_backup.to_csv('Selected_features.csv', index=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Model Building"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [],
   "source": [
    "new_X = df[['concave points_worst','perimeter_worst','concave points_mean','radius_worst','perimeter_mean','area_worst','radius_mean','area_mean','concavity_mean','concavity_worst','compactness_mean','compactness_worst','radius_se','perimeter_se','area_se','texture_worst','smoothness_worst']]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>concave points_worst</th>\n",
       "      <th>perimeter_worst</th>\n",
       "      <th>concave points_mean</th>\n",
       "      <th>radius_worst</th>\n",
       "      <th>perimeter_mean</th>\n",
       "      <th>area_worst</th>\n",
       "      <th>radius_mean</th>\n",
       "      <th>area_mean</th>\n",
       "      <th>concavity_mean</th>\n",
       "      <th>concavity_worst</th>\n",
       "      <th>compactness_mean</th>\n",
       "      <th>compactness_worst</th>\n",
       "      <th>radius_se</th>\n",
       "      <th>perimeter_se</th>\n",
       "      <th>area_se</th>\n",
       "      <th>texture_worst</th>\n",
       "      <th>smoothness_worst</th>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>id</th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "      <th></th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>842302</th>\n",
       "      <td>0.2654</td>\n",
       "      <td>184.60</td>\n",
       "      <td>0.14710</td>\n",
       "      <td>25.380</td>\n",
       "      <td>122.80</td>\n",
       "      <td>2019.0</td>\n",
       "      <td>17.99</td>\n",
       "      <td>1001.0</td>\n",
       "      <td>0.30010</td>\n",
       "      <td>0.7119</td>\n",
       "      <td>0.27760</td>\n",
       "      <td>0.66560</td>\n",
       "      <td>1.0950</td>\n",
       "      <td>8.589</td>\n",
       "      <td>153.40</td>\n",
       "      <td>17.33</td>\n",
       "      <td>0.16220</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>842517</th>\n",
       "      <td>0.1860</td>\n",
       "      <td>158.80</td>\n",
       "      <td>0.07017</td>\n",
       "      <td>24.990</td>\n",
       "      <td>132.90</td>\n",
       "      <td>1956.0</td>\n",
       "      <td>20.57</td>\n",
       "      <td>1326.0</td>\n",
       "      <td>0.08690</td>\n",
       "      <td>0.2416</td>\n",
       "      <td>0.07864</td>\n",
       "      <td>0.18660</td>\n",
       "      <td>0.5435</td>\n",
       "      <td>3.398</td>\n",
       "      <td>74.08</td>\n",
       "      <td>23.41</td>\n",
       "      <td>0.12380</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84300903</th>\n",
       "      <td>0.2430</td>\n",
       "      <td>152.50</td>\n",
       "      <td>0.12790</td>\n",
       "      <td>23.570</td>\n",
       "      <td>130.00</td>\n",
       "      <td>1709.0</td>\n",
       "      <td>19.69</td>\n",
       "      <td>1203.0</td>\n",
       "      <td>0.19740</td>\n",
       "      <td>0.4504</td>\n",
       "      <td>0.15990</td>\n",
       "      <td>0.42450</td>\n",
       "      <td>0.7456</td>\n",
       "      <td>4.585</td>\n",
       "      <td>94.03</td>\n",
       "      <td>25.53</td>\n",
       "      <td>0.14440</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84348301</th>\n",
       "      <td>0.2575</td>\n",
       "      <td>98.87</td>\n",
       "      <td>0.10520</td>\n",
       "      <td>14.910</td>\n",
       "      <td>77.58</td>\n",
       "      <td>567.7</td>\n",
       "      <td>11.42</td>\n",
       "      <td>386.1</td>\n",
       "      <td>0.24140</td>\n",
       "      <td>0.6869</td>\n",
       "      <td>0.28390</td>\n",
       "      <td>0.86630</td>\n",
       "      <td>0.4956</td>\n",
       "      <td>3.445</td>\n",
       "      <td>27.23</td>\n",
       "      <td>26.50</td>\n",
       "      <td>0.20980</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>84358402</th>\n",
       "      <td>0.1625</td>\n",
       "      <td>152.20</td>\n",
       "      <td>0.10430</td>\n",
       "      <td>22.540</td>\n",
       "      <td>135.10</td>\n",
       "      <td>1575.0</td>\n",
       "      <td>20.29</td>\n",
       "      <td>1297.0</td>\n",
       "      <td>0.19800</td>\n",
       "      <td>0.4000</td>\n",
       "      <td>0.13280</td>\n",
       "      <td>0.20500</td>\n",
       "      <td>0.7572</td>\n",
       "      <td>5.438</td>\n",
       "      <td>94.44</td>\n",
       "      <td>16.67</td>\n",
       "      <td>0.13740</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>926424</th>\n",
       "      <td>0.2216</td>\n",
       "      <td>166.10</td>\n",
       "      <td>0.13890</td>\n",
       "      <td>25.450</td>\n",
       "      <td>142.00</td>\n",
       "      <td>2027.0</td>\n",
       "      <td>21.56</td>\n",
       "      <td>1479.0</td>\n",
       "      <td>0.24390</td>\n",
       "      <td>0.4107</td>\n",
       "      <td>0.11590</td>\n",
       "      <td>0.21130</td>\n",
       "      <td>1.1760</td>\n",
       "      <td>7.673</td>\n",
       "      <td>158.70</td>\n",
       "      <td>26.40</td>\n",
       "      <td>0.14100</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>926682</th>\n",
       "      <td>0.1628</td>\n",
       "      <td>155.00</td>\n",
       "      <td>0.09791</td>\n",
       "      <td>23.690</td>\n",
       "      <td>131.20</td>\n",
       "      <td>1731.0</td>\n",
       "      <td>20.13</td>\n",
       "      <td>1261.0</td>\n",
       "      <td>0.14400</td>\n",
       "      <td>0.3215</td>\n",
       "      <td>0.10340</td>\n",
       "      <td>0.19220</td>\n",
       "      <td>0.7655</td>\n",
       "      <td>5.203</td>\n",
       "      <td>99.04</td>\n",
       "      <td>38.25</td>\n",
       "      <td>0.11660</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>926954</th>\n",
       "      <td>0.1418</td>\n",
       "      <td>126.70</td>\n",
       "      <td>0.05302</td>\n",
       "      <td>18.980</td>\n",
       "      <td>108.30</td>\n",
       "      <td>1124.0</td>\n",
       "      <td>16.60</td>\n",
       "      <td>858.1</td>\n",
       "      <td>0.09251</td>\n",
       "      <td>0.3403</td>\n",
       "      <td>0.10230</td>\n",
       "      <td>0.30940</td>\n",
       "      <td>0.4564</td>\n",
       "      <td>3.425</td>\n",
       "      <td>48.55</td>\n",
       "      <td>34.12</td>\n",
       "      <td>0.11390</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>927241</th>\n",
       "      <td>0.2650</td>\n",
       "      <td>184.60</td>\n",
       "      <td>0.15200</td>\n",
       "      <td>25.740</td>\n",
       "      <td>140.10</td>\n",
       "      <td>1821.0</td>\n",
       "      <td>20.60</td>\n",
       "      <td>1265.0</td>\n",
       "      <td>0.35140</td>\n",
       "      <td>0.9387</td>\n",
       "      <td>0.27700</td>\n",
       "      <td>0.86810</td>\n",
       "      <td>0.7260</td>\n",
       "      <td>5.772</td>\n",
       "      <td>86.22</td>\n",
       "      <td>39.42</td>\n",
       "      <td>0.16500</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>92751</th>\n",
       "      <td>0.0000</td>\n",
       "      <td>59.16</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>9.456</td>\n",
       "      <td>47.92</td>\n",
       "      <td>268.6</td>\n",
       "      <td>7.76</td>\n",
       "      <td>181.0</td>\n",
       "      <td>0.00000</td>\n",
       "      <td>0.0000</td>\n",
       "      <td>0.04362</td>\n",
       "      <td>0.06444</td>\n",
       "      <td>0.3857</td>\n",
       "      <td>2.548</td>\n",
       "      <td>19.15</td>\n",
       "      <td>30.37</td>\n",
       "      <td>0.08996</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>569 rows × 17 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "          concave points_worst  perimeter_worst  concave points_mean  \\\n",
       "id                                                                     \n",
       "842302                  0.2654           184.60              0.14710   \n",
       "842517                  0.1860           158.80              0.07017   \n",
       "84300903                0.2430           152.50              0.12790   \n",
       "84348301                0.2575            98.87              0.10520   \n",
       "84358402                0.1625           152.20              0.10430   \n",
       "...                        ...              ...                  ...   \n",
       "926424                  0.2216           166.10              0.13890   \n",
       "926682                  0.1628           155.00              0.09791   \n",
       "926954                  0.1418           126.70              0.05302   \n",
       "927241                  0.2650           184.60              0.15200   \n",
       "92751                   0.0000            59.16              0.00000   \n",
       "\n",
       "          radius_worst  perimeter_mean  area_worst  radius_mean  area_mean  \\\n",
       "id                                                                           \n",
       "842302          25.380          122.80      2019.0        17.99     1001.0   \n",
       "842517          24.990          132.90      1956.0        20.57     1326.0   \n",
       "84300903        23.570          130.00      1709.0        19.69     1203.0   \n",
       "84348301        14.910           77.58       567.7        11.42      386.1   \n",
       "84358402        22.540          135.10      1575.0        20.29     1297.0   \n",
       "...                ...             ...         ...          ...        ...   \n",
       "926424          25.450          142.00      2027.0        21.56     1479.0   \n",
       "926682          23.690          131.20      1731.0        20.13     1261.0   \n",
       "926954          18.980          108.30      1124.0        16.60      858.1   \n",
       "927241          25.740          140.10      1821.0        20.60     1265.0   \n",
       "92751            9.456           47.92       268.6         7.76      181.0   \n",
       "\n",
       "          concavity_mean  concavity_worst  compactness_mean  \\\n",
       "id                                                            \n",
       "842302           0.30010           0.7119           0.27760   \n",
       "842517           0.08690           0.2416           0.07864   \n",
       "84300903         0.19740           0.4504           0.15990   \n",
       "84348301         0.24140           0.6869           0.28390   \n",
       "84358402         0.19800           0.4000           0.13280   \n",
       "...                  ...              ...               ...   \n",
       "926424           0.24390           0.4107           0.11590   \n",
       "926682           0.14400           0.3215           0.10340   \n",
       "926954           0.09251           0.3403           0.10230   \n",
       "927241           0.35140           0.9387           0.27700   \n",
       "92751            0.00000           0.0000           0.04362   \n",
       "\n",
       "          compactness_worst  radius_se  perimeter_se  area_se  texture_worst  \\\n",
       "id                                                                             \n",
       "842302              0.66560     1.0950         8.589   153.40          17.33   \n",
       "842517              0.18660     0.5435         3.398    74.08          23.41   \n",
       "84300903            0.42450     0.7456         4.585    94.03          25.53   \n",
       "84348301            0.86630     0.4956         3.445    27.23          26.50   \n",
       "84358402            0.20500     0.7572         5.438    94.44          16.67   \n",
       "...                     ...        ...           ...      ...            ...   \n",
       "926424              0.21130     1.1760         7.673   158.70          26.40   \n",
       "926682              0.19220     0.7655         5.203    99.04          38.25   \n",
       "926954              0.30940     0.4564         3.425    48.55          34.12   \n",
       "927241              0.86810     0.7260         5.772    86.22          39.42   \n",
       "92751               0.06444     0.3857         2.548    19.15          30.37   \n",
       "\n",
       "          smoothness_worst  \n",
       "id                          \n",
       "842302             0.16220  \n",
       "842517             0.12380  \n",
       "84300903           0.14440  \n",
       "84348301           0.20980  \n",
       "84358402           0.13740  \n",
       "...                    ...  \n",
       "926424             0.14100  \n",
       "926682             0.11660  \n",
       "926954             0.11390  \n",
       "927241             0.16500  \n",
       "92751              0.08996  \n",
       "\n",
       "[569 rows x 17 columns]"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "new_X"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [],
   "source": [
    "scalar = StandardScaler()\n",
    "X_scalar = scalar.fit_transform(new_X)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Knn training Time:  0.01795339584350586\n",
      "Knn test Time    :  0.03179788589477539\n"
     ]
    }
   ],
   "source": [
    "from time import time\n",
    "\n",
    "# Buidling model to test unexposed data\n",
    "x_train,x_test,y_train,y_test = train_test_split(X_scalar,y,test_size = 0.25, random_state= 355)\n",
    "\n",
    "knn = KNeighborsClassifier()\n",
    "\n",
    "# Checking training and testing time (Lazy Learner)\n",
    "start = time()\n",
    "knn.fit(x_train,y_train)\n",
    "print(\"Knn training Time: \", (time() - start))\n",
    "\n",
    "start = time()\n",
    "y_pred = knn.predict(x_test)\n",
    "print(\"Knn test Time    : \",(time() - start))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[91,  2],\n",
       "       [ 4, 46]], dtype=int64)"
      ]
     },
     "execution_count": 19,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfm = confusion_matrix(y_test,y_pred)\n",
    "cfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.98      0.97        93\n",
      "           1       0.96      0.92      0.94        50\n",
      "\n",
      "    accuracy                           0.96       143\n",
      "   macro avg       0.96      0.95      0.95       143\n",
      "weighted avg       0.96      0.96      0.96       143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (classification_report(y_test,y_pred,digits=2))"
   ]
  },
  {
   "attachments": {
    "cross_validation.PNG": {
     "image/png": "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"
    }
   },
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Cross-Validation\n",
    "\n",
    "Suppose you train a model on a given dataset using any specific algorithm. You tried to find the accuracy of the trained model using the same training data and found the accuracy to be 95% or maybe even 100%. What does this mean? Is your model ready for prediction? The answer is no.\n",
    "Why? Because your model has trained itself on the given data, i.e. it knows the data and it has generalized over it very well. But when you try and predict over a new set of data, it’s most likely to give you very bad accuracy, because it has never seen the data before and thus it fails to generalizes well over it. This is the problem of overfitting. \n",
    "To tackle such problem, Cross-validation comes into the picture. Cross-validation is a resampling technique with a basic idea of dividing the training dataset into two parts i.e. train and test. On one part(train) you try to train the model and on the second part(test) i.e. the data which is unseen for the model, you make the prediction and check how well your model works on it. If the model works with good accuracy on your test data, it means that the model has not overfitted the training data and can be trusted with the prediction, whereas if it performs with bad accuracy then our model is not to be trusted and we need to tweak our algorithm.\n",
    "\n",
    "\n",
    "Let’s see the different approaches of Cross-Validation:\n",
    "\n",
    "*\tHold Out Method: \n",
    "\n",
    "It is the most basic of the CV techniques. It simply divides the dataset into two sets of training and test. The training dataset is used to train the model and then test data is fitted in the trained model to make predictions. We check the accuracy and assess our model on that basis. This method is used as it is computationally less costly. But the evaluation based on the Hold-out set can have a high variance because it depends heavily on which data points end up in the training set and which in test data. The evaluation will be different every time this division changes.\n",
    "\n",
    "*\tk-fold Cross-Validation\n",
    "\n",
    "![cross_validation.PNG](attachment:cross_validation.PNG)\n",
    "\n",
    "\n",
    "\n",
    "To tackle the high variance of Hold-out method, the k-fold method is used. The idea is simple, divide the whole dataset into ‘k’ sets preferably of equal sizes. Then the first set is selected as the test set and the rest ‘k-1’ sets are used to train the data. Error is calculated for this particular dataset.\n",
    "Then the steps are repeated, i.e. the second set is selected as the test data, and the remaining ‘k-1’ sets are used as the training data. Again, the error is calculated. Similarly, the process continues for ‘k’ times. In the end, the CV error is given as the mean of the total errors calculated individually, mathematically given as:\n",
    "\n",
    "<img src=\"cv2.png\" width=\"\"> \n",
    "                                               \n",
    "The variance in error decreases with the increase in ‘k’. The disadvantage of k-fold cv is that it is computationally expensive as the algorithm runs from scratch for ‘k’ times.\n",
    "\n",
    "*  Leave One Out Cross Validation (LOOCV)\n",
    "\n",
    "<img src=\"cv3.png\" width=\"\"> \n",
    " \n",
    "LOOCV is a special case of k-fold CV, where k becomes equal to n (number of observations). So instead of creating two subsets, it selects a single observation as a test data and rest of data as the training data. The error is calculated for this test observations. Now, the second observation is selected as test data, and the rest of the data is used as the training set. Again, the error is calculated for this particular test observation. This process continues ‘n’ times and in the end, CV error is calculated as:\n",
    "\n",
    "<img src=\"cv4.png\" width=\"\"> \n",
    "                                             \n",
    "\n",
    "### Bias Variance tradeoff for k-fold CV, LOOCV and Holdout Set CV\n",
    "\n",
    "There is a very good explanation given in the ISLR Book as given below:\n",
    "\n",
    "\n",
    "A k-fold CV with k < n has a computational advantage to LOOCV. But putting computational issues aside,\n",
    "a less obvious but potentially more important advantage of k-fold CV is that it often gives more accurate estimates of the test error rate than does LOOCV.\n",
    "The validation set approach can lead to overestimates of the test error rate since in this approach the\n",
    "the training set used to fit the statistical learning method contains only half the observations of the entire data set. Using this logic, it is not hard to see that LOOCV will give approximately unbiased estimates of the test error since each training set contains n − 1 observations, which is almost as many as the number of observations in the full data set. And performing k-fold CV for, say, k = 5 or k = 10 will lead to an intermediate level of bias since each training set contains (k − 1)n/k observations—fewer than\n",
    "in the LOOCV approach, but substantially more than in the validation set approach. Therefore, from the perspective of bias reduction, it is clear that LOOCV is to be preferred to k-fold CV. However, we know that bias is not the only source for concern in an estimating procedure; we must also consider the procedure’s variance. It turns out that LOOCV has higher variance than does k-fold CV with k < n. Why\n",
    "is this the case? When we perform LOOCV, we are in effect averaging the outputs of n fitted models, each of which is trained on an almost identical set of observations; therefore, these outputs are highly (positively) correlated with each other. In contrast, when we perform k-fold CV with k < n, we are averaging the outputs of k fitted models that are somewhat less correlated with each other since the overlap between the training sets in each model is smaller. Since the mean of many highly correlated quantities has higher variance than does the mean of many quantities that are not as highly correlated, the test error estimate resulting from LOOCV tends to have higher variance than does the test error estimate resulting from k-fold CV.\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Kfold method (for demo)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KFold(n_splits=3, random_state=None, shuffle=True)"
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from sklearn.model_selection import KFold,cross_val_score\n",
    "\n",
    "k_f = KFold(n_splits=3,shuffle=True)\n",
    "\n",
    "k_f"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "train : [0 1 4 5 8 9]   test : [2 3 6 7]\n",
      "train : [2 3 4 6 7 8 9]   test : [0 1 5]\n",
      "train : [0 1 2 3 5 6 7]   test : [4 8 9]\n"
     ]
    }
   ],
   "source": [
    "for train, test in k_f.split([1,2,3,4,5,6,7,8,9,10]):\n",
    "    print ('train :',train,'  test :',test)\n",
    "   "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Cross Validation score to check if the model is overfitting"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.98245614, 0.94736842, 0.94736842, 0.98245614, 0.96491228,\n",
       "       1.        , 0.96491228, 1.        , 0.96491228, 0.96428571])"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(knn, X_scalar, y,cv=10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.9701133364384411"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cross_val_score(KNeighborsClassifier(),X_scalar, y,cv=5).mean()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Hyperparameter Tuning\n",
    "    \n",
    " \n",
    "### Let's Use GridSearchCV for the best parameter to improve the accuracy"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "param_grid = { 'algorithm' : ['kd_tree', 'brute'],\n",
    "               'leaf_size' : [3,5,6,7,8],\n",
    "               'n_neighbors' : [3,5,7,9,11,13]\n",
    "              }"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "gridsearch = GridSearchCV(estimator=knn, param_grid=param_grid)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "GridSearchCV(estimator=KNeighborsClassifier(),\n",
       "             param_grid={'algorithm': ['kd_tree', 'brute'],\n",
       "                         'leaf_size': [3, 5, 6, 7, 8],\n",
       "                         'n_neighbors': [3, 5, 7, 9, 11, 13]})"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridsearch.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'algorithm': 'kd_tree', 'leaf_size': 3, 'n_neighbors': 3}"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gridsearch.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "# we will use the best parameters in our k-NN algorithm and check if accuracy is increasing.\n",
    "knn = KNeighborsClassifier(algorithm = 'kd_tree', leaf_size =3, n_neighbors =3)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "KNeighborsClassifier(algorithm='kd_tree', leaf_size=3, n_neighbors=3)"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "knn.fit(x_train,y_train)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "y_pred = knn.predict(x_test)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([[91,  2],\n",
       "       [ 4, 46]], dtype=int64)"
      ]
     },
     "execution_count": 38,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cfm = confusion_matrix(y_test,y_pred)\n",
    "cfm"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "              precision    recall  f1-score   support\n",
      "\n",
      "           0       0.96      0.98      0.97        93\n",
      "           1       0.96      0.92      0.94        50\n",
      "\n",
      "    accuracy                           0.96       143\n",
      "   macro avg       0.96      0.95      0.95       143\n",
      "weighted avg       0.96      0.96      0.96       143\n",
      "\n"
     ]
    }
   ],
   "source": [
    "print (classification_report(y_test,y_pred,digits=2))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "hide_input": false,
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
